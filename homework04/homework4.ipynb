{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew Thomas\n",
    "4/9/19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Deep neural networks (DNN) have gained significant prominence in the computing world over the last decade. They have been used to solve numerous problems artificially and were even the subject of the 2018 Turing Award. I believe it is hard to argue that they have not been one of the biggest breakthroughs in artificial intelligence, but I would be surprised if they are foundational up to the singularity.\n",
    "\tI think they are a major accomplishment for a couple of reasons. First, most modern data science deals with the recognition of patterns. With the ability to compile numerous features, DNN are very good at this. Image classification, which has been made possible under DNN is a great example. Second, DNNs offer a lot of flexibility, with tons of different ways to configure them.\n",
    "\tDespite this, DNNs do have a problem with overfitting and while they are good at replicating previously seen things, I think there are limits on creativity. Overfitting becomes a problem because as we advance, we will have less tolerance for error and the overfitting problem almost guarantees some error. The other reason I have doubts is that technology changes significantly every decade or so. If DNNs are around for years, they would be some of the longest surviving advances. Due the nature of technology I wouldnâ€™t be surprised if we have a new standard AI tool in 10 years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/aSIbIyp.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype('float32') /255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(1024, activation='tanh', input_shape=(28*28,)))\n",
    "network.add(layers.Dropout(0.01))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy comes out at 0.9750600016593933. You mentioned this was too good to be true, but all that is changed from a \"normal\" (~0.86 accuracy) run and these is the loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
